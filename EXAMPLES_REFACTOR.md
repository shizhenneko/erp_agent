# Examples.txt 重构报告

## 修改日期
2026-01-25

## 问题诊断

### 原始问题
`examples.txt` 存在严重的**硬编码问题**，具体表现为：

1. **高度重合的测试问题**
   - 示例2 与 问题5 完全一致："从前年3月到去年5月，A部门的平均工资"
   - 示例3 与 问题8 完全一致："入职时间一年内、一年到两年、两年到三年的员工最近一个月的平均工资"
   - 示例5 与 问题9 高度相似："涨薪幅度最大的10位员工"
   - 示例6 与 问题10 完全一致："拖欠员工工资的情况"

2. **模型依赖模板匹配而非推理**
   - 模型可能通过关键词匹配直接套用示例SQL
   - 泛化能力未得到真实验证
   - 测试准确率可能虚高

## 解决方案

### 核心策略
将 **"授人以鱼"** 改为 **"授人以渔"**

### 修改内容

#### 1. 使用完全不同的业务场景
- **原来**: 员工、部门、工资表
- **现在**: 虚构的电商场景（订单、产品、客户表）
- **目的**: 避免与实际测试问题的表结构重合

#### 2. 重点教授推理方法
新的示例包括：
- 示例1: 单表统计 - 直接聚合
- 示例2: 时间推理 - 相对日期计算
- 示例3: 多表关联 - JOIN 操作
- 示例4: 分组比较 - CASE WHEN
- 示例5: 分步探索 - 复杂计算
- 示例6: 时间序列分析 - 窗口函数
- 示例7: 异常检测 - 数据完整性检查

#### 3. 强调方法论而非答案
在文件开头明确声明：
```
本文件通过**虚构的业务场景**展示如何使用 ReAct 框架进行数据库查询推理。
重点在于**通用的推理方法和SQL技术**，而非特定问题的答案。
```

#### 4. 保持 ReAct 框架完整性
- 保持 Thought → Action → Observation 的结构
- 展示多轮迭代的推理过程
- 教授何时需要多轮、如何分解问题

## 相关模块修改

### 1. `erp_agent/prompts/examples.txt` ✅
- 完全重写
- 使用电商场景代替员工工资场景
- 新增 SQL 技术速查表
- 新增推理方法论说明

### 2. `erp_agent/prompts/system_prompt.txt` ✅
修改了两处硬编码的示例：
- 第21行：`"涨薪幅度最大的员工是谁？"` → `"对比不同组别的趋势变化"`
- 第145行：`"有没有拖欠工资"` → `"有没有工资异常"`

### 3. 无需修改的模块 ✓
以下模块只是加载 examples.txt 文件，不依赖具体内容：
- `erp_agent/utils/prompt_builder.py` - 只负责加载文件
- `erp_agent/core/sql_generator.py` - 使用 prompt_builder
- `erp_agent/core/agent.py` - 使用 sql_generator
- 测试文件 - 只测试功能，不依赖示例内容

## 验证建议

### 实验方案
为了验证修改的有效性，建议进行以下测试：

#### 测试1: 基线测试
```bash
# 运行完整测试套件，记录当前准确率
python run_tests.py
```

#### 测试2: 泛化能力测试
创建新的测试问题，完全不同于原有的10个问题，例如：
1. "哪些员工连续3个月工资下降？"
2. "每个季度新入职员工的平均起薪是多少？"
3. "找出工资增长率超过部门平均增长率的员工"
4. "哪个月份的总工资支出最高？"
5. "是否有员工的级别与工资不匹配（如级别高但工资低）？"

#### 测试3: 对比实验（可选）
1. 临时恢复旧版 examples.txt
2. 运行问题5、8、9、10
3. 替换为新版 examples.txt
4. 再次运行相同问题
5. 对比准确率变化

**预期结果**:
- 如果准确率**显著下降** → 证明原系统确实依赖硬编码
- 如果准确率**保持稳定** → 证明模型具有真实的推理能力

## 预期影响

### 积极影响
1. ✅ **真实的泛化能力验证** - 测试问题不再泄露在训练数据中
2. ✅ **更强的推理能力** - 模型被迫学习方法而非记忆答案
3. ✅ **更好的可扩展性** - 可以处理更多样的业务场景
4. ✅ **学术诚信** - 实验结果更真实可信

### 潜在风险
1. ⚠️ **短期准确率可能下降** - 尤其是问题5、8、9、10
2. ⚠️ **需要更强的模型能力** - 依赖 LLM 的真实推理能力
3. ⚠️ **可能需要调整 Prompt** - 如果效果不佳，可能需要优化 system_prompt

### 风险缓解措施
1. **增强 system_prompt** - 在 system_prompt.txt 中提供更清晰的推理指导
2. **优化 schema 说明** - 确保数据库结构描述清晰完整
3. **使用更强的模型** - 考虑使用推理能力更强的 LLM
4. **提供业务规则** - 在 system_prompt 中明确业务逻辑

## 后续行动

### 立即执行
- [x] 修改 `examples.txt`
- [x] 修改 `system_prompt.txt` 中的硬编码示例
- [x] 创建本文档

### 建议执行
- [ ] 运行完整测试套件，验证基本功能
- [ ] 设计并执行泛化能力测试
- [ ] 根据测试结果调整 Prompt 策略
- [ ] 更新项目文档，说明改进效果

### 可选执行
- [ ] 进行对比实验，量化改进效果
- [ ] 收集失败案例，分析原因
- [ ] 针对弱项设计更多方法论示例

## 结论

这次重构从根本上改变了系统的学习方式：
- **从"背答案"到"学方法"**
- **从"模板匹配"到"推理生成"**
- **从"特定场景"到"通用能力"**

虽然可能会导致短期准确率下降，但长期来看，这将使系统具有更强的真实能力和更好的可扩展性。

---

**建议**: 在进行重要演示或评估之前，务必进行充分测试，确保系统在新的 examples 下仍能保持可接受的性能水平。如果效果不理想，可以考虑：
1. 在 examples.txt 中增加更多通用方法的示例
2. 在 system_prompt.txt 中提供更详细的业务规则说明
3. 优化时间推理部分的说明，使其更清晰易懂
